{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax import nnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO redo a graphormer achitecture (with a specific envelop term)\n",
    "\n",
    "\"\"\"\n",
    "Model architecture backbone\n",
    "It will be two things : \n",
    "\n",
    "Essentially, it will be the backbone of the transformer world model\n",
    "\n",
    "We will try to code the transformer in jax (flax)\n",
    "\n",
    "Homemade version of the transformer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import einops\n",
    "\n",
    "\n",
    "class FeedForward(nnx.Module):\n",
    "    \"\"\"\n",
    "    Feed forward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        dim_feedforward: int = 2048,\n",
    "        rngs=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=d_model, out_features=dim_feedforward, rngs=rngs\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=dim_feedforward, out_features=d_model, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = nnx.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BiaisMultiHeadAttnetion(nnx.Module):\n",
    "    \"\"\"\n",
    "    This setup is used to input informations from the graph into the attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int = 8,\n",
    "            in_features=512,\n",
    "            qkv_features=512,\n",
    "            rngs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.in_features = in_features\n",
    "        self.qkv_features = qkv_features\n",
    "\n",
    "        # init the layers\n",
    "        self.linear_qkv = nnx.Linear(\n",
    "            in_features=in_features, out_features=qkv_features*3, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # final linear layer \n",
    "        self.linear_last = nnx.Linear(\n",
    "            in_features=in_features, out_features=in_features, rngs=rngs\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, edge):\n",
    "        \"\"\"\n",
    "        x is the node information (nb_batch, seq_len, nb_features)\n",
    "        and edges is (nb_batch, nb_head, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # first pass with linear_qkv\n",
    "        x = self.linear_qkv(x)\n",
    "\n",
    "        # split to go from (nb_batch, seq_len, qkv_features*3) to (nb_batch, nb_head, seq_len, qkv_features // nb_head, 3)  \n",
    "        x = einops.rearrange(\n",
    "            x,\n",
    "            'b s (h f d) -> b h s f d',\n",
    "            h=self.num_heads,\n",
    "            d=3\n",
    "        )\n",
    "\n",
    "        # \n",
    "        query = x[:, :, :, :, 0]\n",
    "        keys = x[:, :, :, :, 1]\n",
    "        values = x[:, :, :, :, 2]\n",
    "\n",
    "        # Compute the dot product between query and keys\n",
    "        qk = jnp.einsum('b h i f, b h j f -> b h i j', query, keys)\n",
    "\n",
    "        # adding biais from edges info\n",
    "        qk = qk + edge\n",
    "\n",
    "        # Scale the dot product by the square root of the feature dimension\n",
    "        qk_scaled = qk / jnp.sqrt(self.qkv_features // self.num_heads)\n",
    "\n",
    "        # Apply softmax to compute attention weights (optional)\n",
    "        attention_weights = jax.nn.softmax(qk_scaled, axis=-1)\n",
    "\n",
    "        # Compute the weighted sum of the values using the attention weights\n",
    "        output = jnp.einsum('b h i j, b h j f -> b h i f', attention_weights, values)\n",
    "\n",
    "        # Concatenate the outputs from all the heads\n",
    "        output = einops.rearrange(output, 'b h s f -> b s (h f)')\n",
    "\n",
    "        return self.linear_last(output)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\"\n",
    "    Transformer block\n",
    "\n",
    "    1. Layer Norm\n",
    "    2. Multi-Head Attention\n",
    "    3. Layer Norm\n",
    "    4. Feed Forward\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.0,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        rngs=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # init layernorm\n",
    "        self.layernorm1 = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "\n",
    "        # init multi-head attention\n",
    "        self.multihead = BiaisMultiHeadAttnetion(\n",
    "            num_heads=nhead,\n",
    "            in_features=d_model,\n",
    "            qkv_features=d_model,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        # init layernorm\n",
    "        self.layernorm2 = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "\n",
    "        # init feed forward\n",
    "        self.feedforward = FeedForward(\n",
    "            d_model=d_model, dim_feedforward=dim_feedforward, rngs=rngs\n",
    "        )\n",
    "\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "    def __call__(self, x, edge):\n",
    "\n",
    "        x_forward = self.layernorm1(x)\n",
    "\n",
    "        x_forward = self.multihead(x_forward, edge)\n",
    "\n",
    "        x_forward = self.dropout(x_forward)\n",
    "        x_forward = x + x_forward\n",
    "        x_forward_second = self.layernorm2(x_forward)\n",
    "        x_forward_second = self.feedforward(x_forward_second)\n",
    "        x_forward_second = self.dropout(x_forward_second)\n",
    "        x_forward_second = x_forward + x_forward_second\n",
    "\n",
    "        return x_forward_second\n",
    "\n",
    "\n",
    "class Transformer(nnx.Module):\n",
    "    \"\"\"\n",
    "    Transformer model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.,\n",
    "        # decoder only\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        out_features: int = 64,\n",
    "        rngs=None,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        # we setup a stack of transformer blocks\n",
    "        self.transformer = nnx.List(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    d_model=d_model,\n",
    "                    nhead=nhead,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    dropout=dropout,\n",
    "                    layer_norm_eps=layer_norm_eps,\n",
    "                    rngs=rngs,\n",
    "                )\n",
    "                for _ in range(num_decoder_layers)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # now the last layer norm and linear layer\n",
    "        self.layernorm = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "        self.linear = nnx.Linear(\n",
    "            in_features=d_model, out_features=out_features, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, edge):\n",
    "\n",
    "        for i in range(self.num_decoder_layers):\n",
    "            x = self.transformer[i](x, edge)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_batch = 32\n",
    "seq_len = 12\n",
    "qkv_features = 512\n",
    "nb_head=8\n",
    "\n",
    "rngs = nnx.Rngs(44)\n",
    "\n",
    "### test session\n",
    "model = Transformer(rngs=rngs)\n",
    "\n",
    "from jax import random\n",
    "key = random.key(0)\n",
    "\n",
    "exemple_node = random.normal(key, (nb_batch, seq_len, qkv_features))\n",
    "\n",
    "edge = random.normal(key,(nb_batch, nb_head, seq_len, seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 12, 64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(exemple_node, edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-1.8483702 ,  0.18487331,  2.2878232 , ...,  0.846782  ,\n",
       "          0.84857917, -0.10905278],\n",
       "        [-0.6393625 , -1.0291516 ,  0.94285446, ..., -0.3374763 ,\n",
       "         -0.17158249, -0.51815677],\n",
       "        [-0.62649816,  1.3318139 , -1.9166517 , ...,  0.9280785 ,\n",
       "         -0.56609195, -2.351592  ],\n",
       "        ...,\n",
       "        [-0.50742257,  0.16040553, -1.1405077 , ..., -0.2639426 ,\n",
       "          0.5241131 ,  0.44146177],\n",
       "        [ 0.5251477 ,  0.656969  ,  0.33670473, ..., -0.04454711,\n",
       "          0.39400226, -0.4205947 ],\n",
       "        [-0.8347021 , -0.9973777 , -1.1542041 , ...,  0.812793  ,\n",
       "          0.9061063 ,  0.78657585]],\n",
       "\n",
       "       [[ 0.758117  , -0.25351432, -0.13754742, ...,  0.12224361,\n",
       "          1.4125918 , -0.7490418 ],\n",
       "        [-0.6561344 ,  1.6324428 ,  0.52141684, ...,  0.7452573 ,\n",
       "         -0.03740723, -0.8705038 ],\n",
       "        [-1.407909  , -0.75454223,  0.5740301 , ..., -0.91798043,\n",
       "          0.5860514 ,  1.6368006 ],\n",
       "        ...,\n",
       "        [-0.06043135, -1.4515245 ,  0.03387675, ...,  1.0014099 ,\n",
       "          1.7674854 ,  0.62066543],\n",
       "        [-0.9146965 , -0.03903516,  0.41045305, ..., -0.12065555,\n",
       "          0.47907946,  0.56672394],\n",
       "        [-0.7826921 , -0.22666529, -0.61640066, ...,  0.53117746,\n",
       "         -1.603876  ,  0.709677  ]],\n",
       "\n",
       "       [[-1.0148033 ,  0.22026235, -0.24033499, ...,  0.6396722 ,\n",
       "          0.30038062, -0.06399142],\n",
       "        [ 0.8979537 , -0.82367676,  1.6864269 , ...,  0.60394067,\n",
       "         -1.8589791 ,  1.4450397 ],\n",
       "        [ 0.63639915, -1.8943577 ,  2.0042222 , ...,  1.7226354 ,\n",
       "          0.53594327, -0.47123268],\n",
       "        ...,\n",
       "        [-0.7121178 , -1.3010944 ,  0.5871179 , ..., -0.78005445,\n",
       "          0.24000639,  1.6290488 ],\n",
       "        [-0.53830963, -0.8299552 ,  0.57927436, ..., -1.508942  ,\n",
       "         -0.7410371 ,  1.0104392 ],\n",
       "        [-1.0187538 ,  0.49516112,  0.82114226, ..., -0.7814656 ,\n",
       "         -1.5713153 , -1.8416275 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.5335895 ,  0.60316086,  0.00403586, ...,  0.54618657,\n",
       "          1.6863747 ,  0.25048918],\n",
       "        [ 0.61122763,  0.47538635,  1.3077549 , ...,  0.43610916,\n",
       "         -1.2795006 , -1.4042962 ],\n",
       "        [-0.42946655,  0.61664295, -0.7616317 , ..., -1.3062942 ,\n",
       "         -0.72207975, -0.8199955 ],\n",
       "        ...,\n",
       "        [-0.1950505 ,  0.89506066, -1.0568786 , ..., -0.4483827 ,\n",
       "          1.4314778 , -1.5480492 ],\n",
       "        [ 0.06810085,  0.6548726 ,  0.60913336, ...,  1.5453358 ,\n",
       "         -2.4019618 , -0.21661556],\n",
       "        [ 0.09881597,  1.1435066 ,  0.00851038, ..., -0.87311983,\n",
       "          0.2147846 ,  0.8523212 ]],\n",
       "\n",
       "       [[-0.01434376,  0.13691683, -1.8895944 , ..., -0.7867377 ,\n",
       "         -0.5763297 , -0.13967317],\n",
       "        [ 0.22809255, -0.5538822 , -2.0504951 , ..., -0.21881829,\n",
       "         -0.9270878 ,  0.18209943],\n",
       "        [ 0.47621757,  0.8177139 ,  0.5460242 , ...,  0.5913383 ,\n",
       "         -0.7055541 ,  1.334719  ],\n",
       "        ...,\n",
       "        [ 0.93583417, -1.1105133 ,  1.1132938 , ...,  0.04246346,\n",
       "          1.4412583 ,  1.9073604 ],\n",
       "        [-0.5517317 , -0.8400728 , -1.6737539 , ..., -0.5924695 ,\n",
       "         -0.53345275, -1.7192768 ],\n",
       "        [-1.0799712 ,  2.2285762 ,  1.3401424 , ...,  2.358381  ,\n",
       "         -0.8514195 ,  2.072072  ]],\n",
       "\n",
       "       [[ 1.3385663 , -0.1993477 , -0.51336455, ...,  1.2589226 ,\n",
       "          0.22623962, -1.2563103 ],\n",
       "        [-0.27885795, -0.72243315, -1.1120899 , ...,  1.0773673 ,\n",
       "          0.21347755, -0.69717884],\n",
       "        [-0.06004069, -0.05609358,  0.14925145, ...,  1.1525458 ,\n",
       "         -0.07626432, -1.7750511 ],\n",
       "        ...,\n",
       "        [-1.1689801 ,  1.210514  , -1.2411392 , ..., -2.1289778 ,\n",
       "         -0.14921261, -0.922335  ],\n",
       "        [-1.0020987 , -0.54850817,  1.6506214 , ..., -0.17777564,\n",
       "          0.38677147, -0.43021378],\n",
       "        [-0.35844916, -0.7254664 ,  0.04953788, ...,  1.1245826 ,\n",
       "         -0.7440657 , -0.08229239]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemple_node"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
