{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax import nnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO redo a graphormer achitecture (with a specific envelop term)\n",
    "\n",
    "\"\"\"\n",
    "Model architecture backbone\n",
    "It will be two things : \n",
    "\n",
    "Essentially, it will be the backbone of the transformer world model\n",
    "\n",
    "We will try to code the transformer in jax (flax)\n",
    "\n",
    "Homemade version of the transformer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import einops\n",
    "\n",
    "\n",
    "class FeedForward(nnx.Module):\n",
    "    \"\"\"\n",
    "    Feed forward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        dim_feedforward: int = 2048,\n",
    "        rngs=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=d_model, out_features=dim_feedforward, rngs=rngs\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=dim_feedforward, out_features=d_model, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = nnx.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BiaisMultiHeadAttnetion(nnx.Module):\n",
    "    \"\"\"\n",
    "    This setup is used to input informations from the graph into the attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int = 8,\n",
    "            in_features=512,\n",
    "            qkv_features=512,\n",
    "            rngs=None):\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.in_features = in_features\n",
    "        self.qkv_features = qkv_features\n",
    "\n",
    "        # init the layers\n",
    "        self.linear_qkv = nnx.Linear(\n",
    "            in_features=in_features, out_features=qkv_features*3, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # final linear layer \n",
    "        self.linear_last = nnx.Linear(\n",
    "            in_features=in_features, out_features=in_features, rngs=rngs\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, edge):\n",
    "        \"\"\"\n",
    "        x is the node information (nb_batch, seq_len, nb_features)\n",
    "        and edges is (nb_batch, nb_head, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # first pass with linear_qkv\n",
    "        x = self.linear_qkv(x)\n",
    "\n",
    "        # split to go from (nb_batch, seq_len, qkv_features*3) to (nb_batch, nb_head, seq_len, qkv_features // nb_head, 3)  \n",
    "        x = einops.rearrange(\n",
    "            x,\n",
    "            'b s (h f d) -> b h s f d',\n",
    "            h=self.nb_head,\n",
    "            d=3\n",
    "        )\n",
    "\n",
    "        # \n",
    "        query = x[:, :, :, :, 0]\n",
    "        keys = x[:, :, :, :, 1]\n",
    "        values = x[:, :, :, :, 2]\n",
    "\n",
    "        # Compute the dot product between query and keys\n",
    "        qk = jnp.einsum('b h i f, b h j f -> b h i j', query, keys)\n",
    "\n",
    "        # adding biais from edges info\n",
    "        qk = qk + edge\n",
    "\n",
    "        # Scale the dot product by the square root of the feature dimension\n",
    "        qk_scaled = qk / jnp.sqrt(self.qkv_features // self.nb_head)\n",
    "\n",
    "        # Apply softmax to compute attention weights (optional)\n",
    "        attention_weights = jax.nn.softmax(qk_scaled, axis=-1)\n",
    "\n",
    "        # Compute the weighted sum of the values using the attention weights\n",
    "        output = jnp.einsum('b h i j, b h j f -> b h i f', attention_weights, values)\n",
    "\n",
    "        # Concatenate the outputs from all the heads\n",
    "        output = einops.rearrange(output, 'b h s f -> b s (h f)')\n",
    "\n",
    "        return self.linear_last(output)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\"\n",
    "    Transformer block\n",
    "\n",
    "    1. Layer Norm\n",
    "    2. Multi-Head Attention\n",
    "    3. Layer Norm\n",
    "    4. Feed Forward\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.0,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        rngs=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # init layernorm\n",
    "        self.layernorm1 = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "\n",
    "        # init multi-head attention\n",
    "        self.multihead = BiaisMultiHeadAttnetion(\n",
    "            num_heads=nhead,\n",
    "            in_features=d_model,\n",
    "            qkv_features=d_model,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        # init layernorm\n",
    "        self.layernorm2 = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "\n",
    "        # init feed forward\n",
    "        self.feedforward = FeedForward(\n",
    "            d_model=d_model, dim_feedforward=dim_feedforward, rngs=rngs\n",
    "        )\n",
    "\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "    def __call__(self, x, edge):\n",
    "        x_forward = self.layernorm1(x)\n",
    "\n",
    "        x_forward = self.multihead(x_forward, edge)\n",
    "\n",
    "        x_forward = self.dropout(x_forward)\n",
    "        x_forward = x + x_forward\n",
    "        x_forward_second = self.layernorm2(x_forward)\n",
    "        x_forward_second = self.feedforward(x_forward_second)\n",
    "        x_forward_second = self.dropout(x_forward_second)\n",
    "        x_forward_second = x_forward + x_forward_second\n",
    "\n",
    "        return x_forward_second\n",
    "\n",
    "\n",
    "class Transformer(nnx.Module):\n",
    "    \"\"\"\n",
    "    Transformer model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.,\n",
    "        # decoder only\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        out_features: int = 64,\n",
    "        rngs=None,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        # we setup a stack of transformer blocks\n",
    "        self.transformer = nnx.List(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    d_model=d_model,\n",
    "                    nhead=nhead,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    dropout=dropout,\n",
    "                    layer_norm_eps=layer_norm_eps,\n",
    "                    rngs=rngs,\n",
    "                )\n",
    "                for _ in range(num_decoder_layers)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # now the last layer norm and linear layer\n",
    "        self.layernorm = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "        self.linear = nnx.Linear(\n",
    "            in_features=d_model, out_features=out_features, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, edge):\n",
    "\n",
    "        for i in range(self.num_decoder_layers):\n",
    "            x = self.transformer[i](x, edge)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
