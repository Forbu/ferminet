{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax import nnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO redo a graphormer achitecture (with a specific envelop term)\n",
    "\n",
    "\"\"\"\n",
    "Model architecture backbone\n",
    "It will be two things : \n",
    "\n",
    "Essentially, it will be the backbone of the transformer world model\n",
    "\n",
    "We will try to code the transformer in jax (flax)\n",
    "\n",
    "Homemade version of the transformer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class FeedForward(nnx.Module):\n",
    "    \"\"\"\n",
    "    Feed forward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        dim_feedforward: int = 2048,\n",
    "        rngs=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=d_model, out_features=dim_feedforward, rngs=rngs\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=dim_feedforward, out_features=d_model, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = nnx.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BiaisMultiHeadAttnetion(nnx.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\"\n",
    "    Transformer block\n",
    "\n",
    "    1. Layer Norm\n",
    "    2. Multi-Head Attention\n",
    "    3. Layer Norm\n",
    "    4. Feed Forward\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.0,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        rngs=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # init layernorm\n",
    "        self.layernorm1 = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "\n",
    "        # init multi-head attention\n",
    "        self.multihead = BiaisMultiHeadAttnetion(\n",
    "            num_heads=nhead,\n",
    "            in_features=d_model,\n",
    "            qkv_features=d_model,\n",
    "            decode=False,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        # init layernorm\n",
    "        self.layernorm2 = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "\n",
    "        # init feed forward\n",
    "        self.feedforward = FeedForward(\n",
    "            d_model=d_model, dim_feedforward=dim_feedforward, rngs=rngs\n",
    "        )\n",
    "\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "    def __call__(self, x, edge):\n",
    "        x_forward = self.layernorm1(x)\n",
    "\n",
    "        x_forward = self.multihead(x_forward, edge)\n",
    "\n",
    "        x_forward = self.dropout(x_forward)\n",
    "        x_forward = x + x_forward\n",
    "        x_forward_second = self.layernorm2(x_forward)\n",
    "        x_forward_second = self.feedforward(x_forward_second)\n",
    "        x_forward_second = self.dropout(x_forward_second)\n",
    "        x_forward_second = x_forward + x_forward_second\n",
    "\n",
    "        return x_forward_second\n",
    "\n",
    "\n",
    "class Transformer(nnx.Module):\n",
    "    \"\"\"\n",
    "    Transformer model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.,\n",
    "        # decoder only\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        out_features: int = 64,\n",
    "        rngs=None,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        # we setup a stack of transformer blocks\n",
    "        self.transformer = nnx.List(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    d_model=d_model,\n",
    "                    nhead=nhead,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    dropout=dropout,\n",
    "                    layer_norm_eps=layer_norm_eps,\n",
    "                    rngs=rngs,\n",
    "                )\n",
    "                for _ in range(num_decoder_layers)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # now the last layer norm and linear layer\n",
    "        self.layernorm = nnx.LayerNorm(num_features=d_model, rngs=rngs)\n",
    "        self.linear = nnx.Linear(\n",
    "            in_features=d_model, out_features=out_features, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, edge):\n",
    "\n",
    "        for i in range(self.num_decoder_layers):\n",
    "            x = self.transformer[i](x, edge)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
